{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "abe2db9701b84034a22b10218f0df304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e60982b0f1cd442b83412513e6d839e8",
              "IPY_MODEL_760439f895e64756ad372462d41283e1",
              "IPY_MODEL_00c8dcd0f3524a5dbde4f788c334d6d2"
            ],
            "layout": "IPY_MODEL_341344a8cf4d48f898f45ba70a3b89db"
          }
        },
        "e60982b0f1cd442b83412513e6d839e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c9c49276f6540638f94b0495400fcf3",
            "placeholder": "​",
            "style": "IPY_MODEL_b4672d337ae4463688fe79e60470c7c2",
            "value": "Downloading: 100%"
          }
        },
        "760439f895e64756ad372462d41283e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce489d480a7741fa9c87a8a7055b5b35",
            "max": 659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0728540f1b614d7e84fc93fd5148971f",
            "value": 659
          }
        },
        "00c8dcd0f3524a5dbde4f788c334d6d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9691e39f199d43d0a79ad3d5f81c9142",
            "placeholder": "​",
            "style": "IPY_MODEL_7e5ad53f597943fabe5dad43c84479ad",
            "value": " 659/659 [00:00&lt;00:00, 9.78kB/s]"
          }
        },
        "341344a8cf4d48f898f45ba70a3b89db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c9c49276f6540638f94b0495400fcf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4672d337ae4463688fe79e60470c7c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce489d480a7741fa9c87a8a7055b5b35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0728540f1b614d7e84fc93fd5148971f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9691e39f199d43d0a79ad3d5f81c9142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e5ad53f597943fabe5dad43c84479ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ace552384ea475380eb0985621c487a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ecaea3d60b954c80af0eb77589b9ae94",
              "IPY_MODEL_abb1a2ac7df14b3781a40a55dfeb3c57",
              "IPY_MODEL_29f0b98adde742fb9cd6441476f861ec"
            ],
            "layout": "IPY_MODEL_a02433b7f69e48f880f8ccd063e53f73"
          }
        },
        "ecaea3d60b954c80af0eb77589b9ae94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ee80c6fad784b2b99431bb632f92c54",
            "placeholder": "​",
            "style": "IPY_MODEL_d371dd29734347cdbab84859b4a1d735",
            "value": "Downloading: 100%"
          }
        },
        "abb1a2ac7df14b3781a40a55dfeb3c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_146107f49e5047cb8e78fe1738d65d4c",
            "max": 411582858,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f77f123556b44b0be435f16978fab32",
            "value": 411582858
          }
        },
        "29f0b98adde742fb9cd6441476f861ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0096b9438914d169740989ad77e7c6c",
            "placeholder": "​",
            "style": "IPY_MODEL_cef3acba111b4682bd384e0248c38bdb",
            "value": " 412M/412M [00:12&lt;00:00, 39.5MB/s]"
          }
        },
        "a02433b7f69e48f880f8ccd063e53f73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ee80c6fad784b2b99431bb632f92c54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d371dd29734347cdbab84859b4a1d735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "146107f49e5047cb8e78fe1738d65d4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f77f123556b44b0be435f16978fab32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0096b9438914d169740989ad77e7c6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef3acba111b4682bd384e0248c38bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k5fQg1Pm8NMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_e0cg7eI_8a"
      },
      "source": [
        "# Set Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpd2tG1mJZDb"
      },
      "source": [
        "## Some Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t783FWSpJ0Pp"
      },
      "source": [
        "Select IDE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHT6u1UYJuOk"
      },
      "outputs": [],
      "source": [
        "IDE = \"Colab\"\n",
        "# IDE = \"Jupyter\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bantkm_NKARB"
      },
      "source": [
        "Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHjytjE0J52z",
        "outputId": "efe50f70-70d0-4a15-8b28-3d3356f583d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if IDE == \"Colab\":\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R-hI4_BMYEg"
      },
      "source": [
        "Change working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD6wlg99MbxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07be5f1-8a19-4a2e-9c05-feb9ecba0ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dataset\n",
            " Ensemble.ipynb\n",
            "'model_aspect_polarity_balanced_multi_branch.pt.pt 的副本.pt 的副本.pt 的副本'\n",
            " model_hw3_1_2nd_ensemble.ckpt\n",
            " NLP_2022_Spring_Sentiment_Analysis_v2.ipynb\n",
            " old\n",
            " Sentiment_Analysis\n",
            " wandb\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if IDE == \"Colab\":\n",
        "  os.chdir(\"/content/drive/MyDrive/Colab Notebooks\")\n",
        "  !ls\n",
        "else:\n",
        "  !dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NkbUSx7KZZJ"
      },
      "source": [
        "Install packages<br/>\n",
        "% affect globally，! affect locally.<br/>\n",
        "https://stackoverflow.com/a/57212513/3513289<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnnFtJB_Kdmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346d4626-17ed-45d1-da7c-06825e2fffcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Collecting PyYAML\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.17-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 65.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=4cac3841124632ff17c9b86e4458220b7da66195fe7921d674fc9d2398eaaf7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.17\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://github.com/SimpleITK/SimpleITK/releases/tag/latest\n",
            "Collecting SimpleITK\n",
            "  Downloading https://github.com/SimpleITK/SimpleITK/releases/download/latest/SimpleITK-2.2.0rc3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 53.0 MB 51 kB/s \n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.2.0rc3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (5.5.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly) (8.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.5.0\n",
            "  Downloading transformers-4.5.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (3.7.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 31.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.11.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.64.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.0) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2022.5.18.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=685d4e59c44379c143b3dc61efcc2535e6c224bc1ac2683407e86ed3236b6973\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.5.0\n"
          ]
        }
      ],
      "source": [
        "if IDE == \"Colab\":\n",
        "  %pip install --upgrade PyYAML\n",
        "  %pip install wandb\n",
        "  %pip install --upgrade scikit-learn\n",
        "  %pip install --upgrade --pre SimpleITK --find-links https://github.com/SimpleITK/SimpleITK/releases/tag/latest\n",
        "  %pip install plotly\n",
        "  %pip install timm\n",
        "  %pip install transformers==4.5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSGdvjBzM1Kg"
      },
      "source": [
        "Import packages<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X48UC_nKMyX1"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages.\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
        "from torch.utils.data import ConcatDataset, Dataset, DataLoader, Subset\n",
        "from torchvision.datasets import DatasetFolder\n",
        "# This is for the progress bar.\n",
        "# https://discuss.pytorch.org/t/error-while-multiprocessing-in-dataloader/46845/9\n",
        "# import tqdm.auto as tqdm\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from datetime import datetime\n",
        "import asyncio\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import math\n",
        "from sklearn import manifold\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import pdb\n",
        "import re\n",
        "import wandb\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import collections\n",
        "# https://stackoverflow.com/questions/47775621/interpolate-resize-3d-array\n",
        "from scipy.ndimage import zoom, gaussian_filter\n",
        "from scipy import signal\n",
        "import importlib\n",
        "import timm\n",
        "from random import sample, shuffle, randrange\n",
        "from transformers import AdamW, BertModel, BertForQuestionAnswering, BertForSequenceClassification, BertTokenizerFast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpFCnlBUNSMH"
      },
      "source": [
        "## Set global variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2EjKHTuOU_B"
      },
      "source": [
        "Set global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkf5qte6NPRT"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "20220520 Andy\n",
        "\"batch_size\": 8在Tesla P100 16G會CUDA out of memory\n",
        "\n",
        "20220529 Andy\n",
        "chinese-macbert-large在resume時，會CUDA out of memory，\n",
        "改用chinese-macbert-base可解決。\n",
        "\"\"\"\n",
        "global_config = {\n",
        "    \"batch_size\": 4\n",
        "    , \"learning_rate\": 5e-5\n",
        "    , \"epoch_num\": 50\n",
        "    , \"early_stop\": 20\n",
        "    , \"max_seq_len\": 512\n",
        "    # , \"BERT_model\": \"hfl/chinese-macbert-large\"\n",
        "    , \"BERT_model\": \"hfl/chinese-macbert-base\"\n",
        "    , \"doc_stride\": 45\n",
        "}\n",
        "USE_WANDB = True\n",
        "DOWNLOAD = False\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "POLARITY_TXT = [\"not_mentioned\", \"negative\", \"neutral\", \"positive\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO4vSN2kOOC5"
      },
      "source": [
        "Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIKJYvfpONd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d7b7bf-0d8b-40b5-80fe-0a9d1b206a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is cuda\n",
            "Wed Jun  8 06:27:17 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    12W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device is {device}\")\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC_z1FEHOkoE"
      },
      "source": [
        "## Set wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIS_92GKPkGJ"
      },
      "source": [
        "wandb resume:\n",
        "1. Change RESUME to True.\n",
        "2. Replace run_id with the previous one.\n",
        "https://docs.wandb.ai/guides/track/advanced/resuming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeBX1c36ORGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9803e916-b7af-4a23-a0c2-d5ce247dc52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2ygwnfy0\n"
          ]
        }
      ],
      "source": [
        "RESUME = True\n",
        "if RESUME:\n",
        "  run_id = \"2ygwnfy0\"\n",
        "else:\n",
        "  run_id = wandb.util.generate_id()\n",
        "print(run_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_rw9IBNPout"
      },
      "source": [
        "wandb initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtCWZXVMO_Xz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ca550d62-3e06-44e6-f814-52e13e046de2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcyx6\u001b[0m (\u001b[33mnlp_2022_spring_sentiment_analysis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Colab Notebooks/wandb/run-20220608_062721-2ygwnfy0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Resuming run <strong><a href=\"https://wandb.ai/nlp_2022_spring_sentiment_analysis/Sentiment_Analysis/runs/2ygwnfy0\" target=\"_blank\">20220603-0657_aspect_polarity_balanced_multi_branch</a></strong> to <a href=\"https://wandb.ai/nlp_2022_spring_sentiment_analysis/Sentiment_Analysis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "if IDE == \"Jupyter\":\n",
        "  os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Sentiment_Analysis\"\n",
        "brief_name = \"aspect_polarity_balanced_multi_branch\"\n",
        "if USE_WANDB:\n",
        "  wandb.login()\n",
        "  wandb.init(project=\"Sentiment_Analysis\", id=run_id, resume=\"allow\", config=global_config, entity=\"nlp_2022_spring_sentiment_analysis\")\n",
        "  if not RESUME:\n",
        "    wandb.run.name = datetime.today().strftime(f\"%Y%m%d-%H%M_{brief_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07OUqwecLz1C"
      },
      "source": [
        "## Set random seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7atXRm7L3jp"
      },
      "outputs": [],
      "source": [
        "def SetSeeds(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  np.random.seed(seed)\n",
        "SetSeeds(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ6lrkZe7UUo"
      },
      "outputs": [],
      "source": [
        "class AspectSentimentAnalysis_branch(nn.Module):\n",
        "  def __init__(self, max_seq_len, polarity_num):\n",
        "    super().__init__()\n",
        "\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.hidden_size = 768\n",
        "    self.hidden_dim = -1\n",
        "    self.polarity_num = polarity_num\n",
        "\n",
        "    # Aspect attention pooling\n",
        "    self.attention_pooling = nn.Sequential()\n",
        "    self.attention_pooling.add_module(name=\"att_fc_in_1\", module=nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size))\n",
        "    self.attention_pooling.add_module(name=\"att_act\", module=nn.Tanh())\n",
        "    self.attention_pooling.add_module(name=\"att_fc_out\", module=nn.Linear(in_features=self.hidden_size, out_features=1, bias=False))\n",
        "    self.attention_pooling.add_module(name=\"att_softmax\", module=nn.Softmax(dim=self.hidden_dim))\n",
        "\n",
        "    # Final calculation of attention\n",
        "    self.att_fc_in_2 = nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
        "    self.att_act = nn.Tanh().to(device)\n",
        "\n",
        "    # Polarity classifier\n",
        "    self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.polarity_num)\n",
        "\n",
        "  def forward(self, last_hidden_states_masked):\n",
        "    \"\"\"\n",
        "    Aspect attention pooling\n",
        "    att_alpha.shape is torch.Size([4, 512, 1])\n",
        "\n",
        "    att_W2.shape is torch.Size([4, 512, 768])\n",
        "    att_W2_T.shape is torch.Size([4, 768, 512])\n",
        "    att_out.shape is torch.Size([4, 768, 1])\n",
        "    att_r.shape is torch.Size([4, 768, 1])\n",
        "    att_r_flat.shape is torch.Size([4, 768])\n",
        "    \"\"\"\n",
        "    att_alpha = self.attention_pooling(last_hidden_states_masked)\n",
        "    att_W2 = self.att_fc_in_2(last_hidden_states_masked)\n",
        "    # print(f\"att_W2.shape is {att_W2.shape}\")\n",
        "    att_W2_T = att_W2.view(-1, self.hidden_size, self.max_seq_len)\n",
        "    # print(f\"att_W2_T.shape is {att_W2_T.shape}\")\n",
        "    att_out = torch.matmul(att_W2_T, att_alpha)\n",
        "    # print(f\"att_out.shape is {att_out.shape}\")\n",
        "    att_r = self.att_act(att_out)\n",
        "    # print(f\"att_r.shape is {att_r.shape}\")\n",
        "    att_r_flat = att_r.view(-1, self.hidden_size)\n",
        "    # print(f\"att_r_flat.shape is {att_r_flat.shape}\")\n",
        "    # print()\n",
        "\n",
        "    \"\"\"\n",
        "    Polarity classifier \n",
        "    logits.shape is torch.Size([4, 4])\n",
        "    \"\"\" \n",
        "    logits = self.fc_out(att_r_flat)\n",
        "    # print(f\"logits.shape is {logits.shape}\")\n",
        "    \n",
        "    return logits\n",
        "\n",
        "class AspectSentimentAnalysis_main(nn.Module):\n",
        "  def __init__(self, device, pretrain_bert_name, max_seq_len, aspect_num, polarity_num):\n",
        "    super().__init__()\n",
        "\n",
        "    # Bert feature extractor\n",
        "    self.device = device\n",
        "    self.bert = BertModel.from_pretrained(pretrain_bert_name)\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.aspect_num = aspect_num\n",
        "    self.hidden_size = 768\n",
        "    self.hidden_dim = -1\n",
        "    self.batch_repeat = 1\n",
        "    self.review_repeat = 1\n",
        "    self.polarity_num = polarity_num\n",
        "    self.word_dim = 2\n",
        "\n",
        "    \"\"\"\n",
        "    20220605 Andy\n",
        "    不可用單純的python list存放module！這樣會無法正常訓練。\n",
        "    multi-branch情況下，必須使用ModuleList。\n",
        "    可觀察print(model)的結果，確認module是否被納入。\n",
        "    When should I use nn.ModuleList and when should I use nn.Sequential\n",
        "    https://discuss.pytorch.org/t/when-should-i-use-nn-modulelist-and-when-should-i-use-nn-sequential/5463/4\n",
        "    \"\"\"\n",
        "    # Aspect polarity classifier\n",
        "    self.aspect_branch_list = nn.ModuleList()\n",
        "    for aspect in range(self.aspect_num):\n",
        "      self.aspect_branch_list.append(AspectSentimentAnalysis_branch(max_seq_len=max_seq_len, polarity_num=polarity_num).to(self.device))\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    \"\"\"\n",
        "    Forward batch through bert\n",
        "    last_hidden_states.shape is torch.Size([4, 512, 768])\n",
        "    \"\"\"\n",
        "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    pooler_output  = outputs.pooler_output \n",
        "    # print(f\"last_hidden_states.shape is {last_hidden_states.shape}\")\n",
        "    # print(f\"last_hidden_states is {last_hidden_states}\")\n",
        "    # print()\n",
        "\n",
        "    \"\"\"\n",
        "    Apply attention_mask to last_hidden_states\n",
        "    attention_mask.shape is torch.Size([4, 512])\n",
        "    attention_mask_unsqueeze.shape is torch.Size([4, 512, 1])\n",
        "    attention_mask_repeat.shape is torch.Size([4, 512, 768])\n",
        "    last_hidden_states_masked.shape is torch.Size([4, 512, 768])\n",
        "    \"\"\"\n",
        "    batch_len, review_len, word_len = last_hidden_states.shape\n",
        "    attention_mask_unsqueeze = torch.unsqueeze(attention_mask, dim=self.word_dim)\n",
        "    # print(f\"attention_mask_unsqueeze.shape is {attention_mask_unsqueeze.shape}\")\n",
        "    attention_mask_repeat = attention_mask_unsqueeze.repeat(self.batch_repeat, self.review_repeat, self.hidden_size)\n",
        "    # print(f\"attention_mask_repeat.shape is {attention_mask_repeat.shape}\")\n",
        "    # print(f\"attention_mask_repeat[0] is {attention_mask_repeat[0]}\")\n",
        "    last_hidden_states_masked = last_hidden_states * attention_mask_repeat\n",
        "    # print(f\"last_hidden_states_masked.shape is {last_hidden_states_masked.shape}\")\n",
        "    # print(f\"last_hidden_states_masked[0] is {last_hidden_states_masked[0]}\")\n",
        "    # print()    \n",
        "\n",
        "    \"\"\"\n",
        "    Aspect polarity classifier \n",
        "    \"\"\" \n",
        "    aspect_logits_list = []\n",
        "    for aspect in range(self.aspect_num):\n",
        "      # logits.shape is torch.Size([4, 4])\n",
        "      logits = self.aspect_branch_list[aspect](last_hidden_states_masked)\n",
        "      # print(f\"logits.shape is {logits.shape}\")\n",
        "      aspect_logits_list.append(logits)\n",
        "    \n",
        "    return aspect_logits_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path_list = [\n",
        "  \"/content/drive/MyDrive/Colab Notebooks/model_aspect_polarity_balanced_multi_branch.pt.pt 的副本.pt 的副本.pt 的副本\", # 0.939, 無augmentation\n",
        "  \"/content/drive/MyDrive/Colab Notebooks/model_aspect_polarity_balanced_multi_branch.pt.pt 的副本.pt 的副本.pt 的副本\", # 0.939, augmentation\n",
        "  # \"model_hw3_1_2nd_20211211-1245.pt\", # 0.934, lr decay\n",
        "]\n",
        "\"\"\"\n",
        "result: \n",
        "0.939 + 0.939 + 0.934 = 0.9386\n",
        "0.939 + 0.939 = 0.943\n",
        "\"\"\"\n",
        "model_list = []\n",
        "for model_path in model_path_list:\n",
        "  print(f\"Loading model {model_path}\")\n",
        "  model_base = AspectSentimentAnalysis_main(device=device, pretrain_bert_name=global_config[\"BERT_model\"], max_seq_len=global_config[\"max_seq_len\"], aspect_num=18, polarity_num=len(POLARITY_TXT))\n",
        "  model_base = model_base.to(device)\n",
        "  model_base.load_state_dict(torch.load(model_path))\n",
        "  model_list.append(model_base)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "abe2db9701b84034a22b10218f0df304",
            "e60982b0f1cd442b83412513e6d839e8",
            "760439f895e64756ad372462d41283e1",
            "00c8dcd0f3524a5dbde4f788c334d6d2",
            "341344a8cf4d48f898f45ba70a3b89db",
            "1c9c49276f6540638f94b0495400fcf3",
            "b4672d337ae4463688fe79e60470c7c2",
            "ce489d480a7741fa9c87a8a7055b5b35",
            "0728540f1b614d7e84fc93fd5148971f",
            "9691e39f199d43d0a79ad3d5f81c9142",
            "7e5ad53f597943fabe5dad43c84479ad",
            "7ace552384ea475380eb0985621c487a",
            "ecaea3d60b954c80af0eb77589b9ae94",
            "abb1a2ac7df14b3781a40a55dfeb3c57",
            "29f0b98adde742fb9cd6441476f861ec",
            "a02433b7f69e48f880f8ccd063e53f73",
            "4ee80c6fad784b2b99431bb632f92c54",
            "d371dd29734347cdbab84859b4a1d735",
            "146107f49e5047cb8e78fe1738d65d4c",
            "1f77f123556b44b0be435f16978fab32",
            "d0096b9438914d169740989ad77e7c6c",
            "cef3acba111b4682bd384e0248c38bdb"
          ]
        },
        "id": "qXuib0KL85SM",
        "outputId": "ac368e91-d350-41ef-90ce-f2c3a83e40ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model /content/drive/MyDrive/Colab Notebooks/model_aspect_polarity_balanced_multi_branch.pt.pt 的副本.pt 的副本.pt 的副本\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/659 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abe2db9701b84034a22b10218f0df304"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/412M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ace552384ea475380eb0985621c487a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading model /content/drive/MyDrive/Colab Notebooks/model_aspect_polarity_balanced_multi_branch.pt.pt 的副本.pt 的副本.pt 的副本\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_dict_sum = collections.OrderedDict()\n",
        "params_keys = None\n",
        "params_dict_averaged = collections.OrderedDict()\n",
        "\n",
        "def AddModelPara2Dict(model, model_path):\n",
        "  global params_dict_sum\n",
        "  global params_keys\n",
        "\n",
        "  # Get keys from model\n",
        "  model_state_dict = model.state_dict()\n",
        "  model_params_keys = list(model_state_dict.keys())\n",
        "  if params_keys is None:\n",
        "    params_keys = model_params_keys\n",
        "  elif params_keys != model_params_keys:\n",
        "    raise KeyError(\n",
        "        \"For checkpoint {}, expected list of params: {}, \"\n",
        "        \"but found: {}\".format(model_path, params_keys, model_params_keys)\n",
        "    )\n",
        "\n",
        "  \"\"\"\n",
        "  ['cnn_layers.0.weight', 'cnn_layers.0.bias', 'cnn_layers.1.weight', 'cnn_layers.1.bias', 'cnn_layers.1.running_mean', 'cnn_layers.1.running_var', 'cnn_layers.1.num_batches_tracked', 'cnn_layers.4.weight', 'cnn_layers.4.bias', 'cnn_layers.5.weight', 'cnn_layers.5.bias', 'cnn_layers.5.running_mean', 'cnn_layers.5.running_var', 'cnn_layers.5.num_batches_tracked', 'cnn_layers.8.weight', 'cnn_layers.8.bias', 'cnn_layers.9.weight', 'cnn_layers.9.bias', 'cnn_layers.9.running_mean', 'cnn_layers.9.running_var', 'cnn_layers.9.num_batches_tracked', \n",
        "  'fc_layers.0.weight', 'fc_layers.0.bias', 'fc_layers.2.weight', 'fc_layers.2.bias', 'fc_layers.4.weight', 'fc_layers.4.bias']\n",
        "  \"\"\"\n",
        "  # Sum parameters of different models\n",
        "  for k in params_keys:\n",
        "    p = model_state_dict[k]\n",
        "    if isinstance(p, torch.HalfTensor):\n",
        "      p = p.float()\n",
        "    if k not in params_dict_sum:\n",
        "      params_dict_sum[k] = p.clone()\n",
        "      # NOTE: clone() is needed in case of p is a shared parameter\n",
        "    else:\n",
        "      print(f\"Append {k}\")\n",
        "      params_dict_sum[k] += p\n",
        "\n",
        "\n",
        "# Ensemble can only be performed on the same models\n",
        "\n",
        "# Add parameters of model to params_dict\n",
        "ensemble_len = 0\n",
        "for i, model in enumerate(model_list):\n",
        "  print(f\"Adding model {model_path_list[i]}\")\n",
        "  AddModelPara2Dict(model, model_path_list[i])\n",
        "  ensemble_len += 1\n",
        "\n",
        "# Average parameters\n",
        "for k, v in params_dict_sum.items():\n",
        "  params_dict_averaged[k] = v\n",
        "  print(f\"Divide {k}\")\n",
        "  if params_dict_averaged[k].is_floating_point():\n",
        "    params_dict_averaged[k].div_(ensemble_len)\n",
        "  else:\n",
        "    params_dict_averaged[k] //= ensemble_len\n",
        "\n",
        "# Set parameters back to a new model\n",
        "model_ensemble = AspectSentimentAnalysis_main(device=device, pretrain_bert_name=global_config[\"BERT_model\"], max_seq_len=global_config[\"max_seq_len\"], aspect_num=18, polarity_num=len(POLARITY_TXT))\n",
        "model_ensemble.to(device)\n",
        "model_ensemble.load_state_dict(params_dict_averaged) \n",
        "\n",
        "# Save ensemble model\n",
        "# for k, v in averaged_params:\n",
        "#   model_ensemble[k] = v  \n",
        "model_path_ensemble = \"NEW_model.ckpt\"\n",
        "model_path = model_path_ensemble\n",
        "print(f\"Saving model {model_path_ensemble}\")\n",
        "torch.save(model_ensemble.state_dict(), model_path_ensemble)\n",
        "model_base = model_ensemble"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4VfDJes_-Nl",
        "outputId": "c2bc0c15-fca7-4b32-d775-1e002a8bed2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding model /content/drive/MyDrive/Colab Notebooks/model_aspect_polarity_balanced_multi_branch.pt.pt 的副本.pt 的副本.pt 的副本\n",
            "Adding model /content/drive/MyDrive/Colab Notebooks/model_aspect_polarity_balanced_multi_branch.pt.pt 的副本.pt 的副本.pt 的副本\n",
            "Append bert.embeddings.position_ids\n",
            "Append bert.embeddings.word_embeddings.weight\n",
            "Append bert.embeddings.position_embeddings.weight\n",
            "Append bert.embeddings.token_type_embeddings.weight\n",
            "Append bert.embeddings.LayerNorm.weight\n",
            "Append bert.embeddings.LayerNorm.bias\n",
            "Append bert.encoder.layer.0.attention.self.query.weight\n",
            "Append bert.encoder.layer.0.attention.self.query.bias\n",
            "Append bert.encoder.layer.0.attention.self.key.weight\n",
            "Append bert.encoder.layer.0.attention.self.key.bias\n",
            "Append bert.encoder.layer.0.attention.self.value.weight\n",
            "Append bert.encoder.layer.0.attention.self.value.bias\n",
            "Append bert.encoder.layer.0.attention.output.dense.weight\n",
            "Append bert.encoder.layer.0.attention.output.dense.bias\n",
            "Append bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.0.intermediate.dense.weight\n",
            "Append bert.encoder.layer.0.intermediate.dense.bias\n",
            "Append bert.encoder.layer.0.output.dense.weight\n",
            "Append bert.encoder.layer.0.output.dense.bias\n",
            "Append bert.encoder.layer.0.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.0.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.1.attention.self.query.weight\n",
            "Append bert.encoder.layer.1.attention.self.query.bias\n",
            "Append bert.encoder.layer.1.attention.self.key.weight\n",
            "Append bert.encoder.layer.1.attention.self.key.bias\n",
            "Append bert.encoder.layer.1.attention.self.value.weight\n",
            "Append bert.encoder.layer.1.attention.self.value.bias\n",
            "Append bert.encoder.layer.1.attention.output.dense.weight\n",
            "Append bert.encoder.layer.1.attention.output.dense.bias\n",
            "Append bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.1.intermediate.dense.weight\n",
            "Append bert.encoder.layer.1.intermediate.dense.bias\n",
            "Append bert.encoder.layer.1.output.dense.weight\n",
            "Append bert.encoder.layer.1.output.dense.bias\n",
            "Append bert.encoder.layer.1.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.1.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.2.attention.self.query.weight\n",
            "Append bert.encoder.layer.2.attention.self.query.bias\n",
            "Append bert.encoder.layer.2.attention.self.key.weight\n",
            "Append bert.encoder.layer.2.attention.self.key.bias\n",
            "Append bert.encoder.layer.2.attention.self.value.weight\n",
            "Append bert.encoder.layer.2.attention.self.value.bias\n",
            "Append bert.encoder.layer.2.attention.output.dense.weight\n",
            "Append bert.encoder.layer.2.attention.output.dense.bias\n",
            "Append bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.2.intermediate.dense.weight\n",
            "Append bert.encoder.layer.2.intermediate.dense.bias\n",
            "Append bert.encoder.layer.2.output.dense.weight\n",
            "Append bert.encoder.layer.2.output.dense.bias\n",
            "Append bert.encoder.layer.2.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.2.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.3.attention.self.query.weight\n",
            "Append bert.encoder.layer.3.attention.self.query.bias\n",
            "Append bert.encoder.layer.3.attention.self.key.weight\n",
            "Append bert.encoder.layer.3.attention.self.key.bias\n",
            "Append bert.encoder.layer.3.attention.self.value.weight\n",
            "Append bert.encoder.layer.3.attention.self.value.bias\n",
            "Append bert.encoder.layer.3.attention.output.dense.weight\n",
            "Append bert.encoder.layer.3.attention.output.dense.bias\n",
            "Append bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.3.intermediate.dense.weight\n",
            "Append bert.encoder.layer.3.intermediate.dense.bias\n",
            "Append bert.encoder.layer.3.output.dense.weight\n",
            "Append bert.encoder.layer.3.output.dense.bias\n",
            "Append bert.encoder.layer.3.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.3.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.4.attention.self.query.weight\n",
            "Append bert.encoder.layer.4.attention.self.query.bias\n",
            "Append bert.encoder.layer.4.attention.self.key.weight\n",
            "Append bert.encoder.layer.4.attention.self.key.bias\n",
            "Append bert.encoder.layer.4.attention.self.value.weight\n",
            "Append bert.encoder.layer.4.attention.self.value.bias\n",
            "Append bert.encoder.layer.4.attention.output.dense.weight\n",
            "Append bert.encoder.layer.4.attention.output.dense.bias\n",
            "Append bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.4.intermediate.dense.weight\n",
            "Append bert.encoder.layer.4.intermediate.dense.bias\n",
            "Append bert.encoder.layer.4.output.dense.weight\n",
            "Append bert.encoder.layer.4.output.dense.bias\n",
            "Append bert.encoder.layer.4.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.4.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.5.attention.self.query.weight\n",
            "Append bert.encoder.layer.5.attention.self.query.bias\n",
            "Append bert.encoder.layer.5.attention.self.key.weight\n",
            "Append bert.encoder.layer.5.attention.self.key.bias\n",
            "Append bert.encoder.layer.5.attention.self.value.weight\n",
            "Append bert.encoder.layer.5.attention.self.value.bias\n",
            "Append bert.encoder.layer.5.attention.output.dense.weight\n",
            "Append bert.encoder.layer.5.attention.output.dense.bias\n",
            "Append bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.5.intermediate.dense.weight\n",
            "Append bert.encoder.layer.5.intermediate.dense.bias\n",
            "Append bert.encoder.layer.5.output.dense.weight\n",
            "Append bert.encoder.layer.5.output.dense.bias\n",
            "Append bert.encoder.layer.5.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.5.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.6.attention.self.query.weight\n",
            "Append bert.encoder.layer.6.attention.self.query.bias\n",
            "Append bert.encoder.layer.6.attention.self.key.weight\n",
            "Append bert.encoder.layer.6.attention.self.key.bias\n",
            "Append bert.encoder.layer.6.attention.self.value.weight\n",
            "Append bert.encoder.layer.6.attention.self.value.bias\n",
            "Append bert.encoder.layer.6.attention.output.dense.weight\n",
            "Append bert.encoder.layer.6.attention.output.dense.bias\n",
            "Append bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.6.intermediate.dense.weight\n",
            "Append bert.encoder.layer.6.intermediate.dense.bias\n",
            "Append bert.encoder.layer.6.output.dense.weight\n",
            "Append bert.encoder.layer.6.output.dense.bias\n",
            "Append bert.encoder.layer.6.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.6.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.7.attention.self.query.weight\n",
            "Append bert.encoder.layer.7.attention.self.query.bias\n",
            "Append bert.encoder.layer.7.attention.self.key.weight\n",
            "Append bert.encoder.layer.7.attention.self.key.bias\n",
            "Append bert.encoder.layer.7.attention.self.value.weight\n",
            "Append bert.encoder.layer.7.attention.self.value.bias\n",
            "Append bert.encoder.layer.7.attention.output.dense.weight\n",
            "Append bert.encoder.layer.7.attention.output.dense.bias\n",
            "Append bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.7.intermediate.dense.weight\n",
            "Append bert.encoder.layer.7.intermediate.dense.bias\n",
            "Append bert.encoder.layer.7.output.dense.weight\n",
            "Append bert.encoder.layer.7.output.dense.bias\n",
            "Append bert.encoder.layer.7.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.7.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.8.attention.self.query.weight\n",
            "Append bert.encoder.layer.8.attention.self.query.bias\n",
            "Append bert.encoder.layer.8.attention.self.key.weight\n",
            "Append bert.encoder.layer.8.attention.self.key.bias\n",
            "Append bert.encoder.layer.8.attention.self.value.weight\n",
            "Append bert.encoder.layer.8.attention.self.value.bias\n",
            "Append bert.encoder.layer.8.attention.output.dense.weight\n",
            "Append bert.encoder.layer.8.attention.output.dense.bias\n",
            "Append bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.8.intermediate.dense.weight\n",
            "Append bert.encoder.layer.8.intermediate.dense.bias\n",
            "Append bert.encoder.layer.8.output.dense.weight\n",
            "Append bert.encoder.layer.8.output.dense.bias\n",
            "Append bert.encoder.layer.8.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.8.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.9.attention.self.query.weight\n",
            "Append bert.encoder.layer.9.attention.self.query.bias\n",
            "Append bert.encoder.layer.9.attention.self.key.weight\n",
            "Append bert.encoder.layer.9.attention.self.key.bias\n",
            "Append bert.encoder.layer.9.attention.self.value.weight\n",
            "Append bert.encoder.layer.9.attention.self.value.bias\n",
            "Append bert.encoder.layer.9.attention.output.dense.weight\n",
            "Append bert.encoder.layer.9.attention.output.dense.bias\n",
            "Append bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.9.intermediate.dense.weight\n",
            "Append bert.encoder.layer.9.intermediate.dense.bias\n",
            "Append bert.encoder.layer.9.output.dense.weight\n",
            "Append bert.encoder.layer.9.output.dense.bias\n",
            "Append bert.encoder.layer.9.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.9.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.10.attention.self.query.weight\n",
            "Append bert.encoder.layer.10.attention.self.query.bias\n",
            "Append bert.encoder.layer.10.attention.self.key.weight\n",
            "Append bert.encoder.layer.10.attention.self.key.bias\n",
            "Append bert.encoder.layer.10.attention.self.value.weight\n",
            "Append bert.encoder.layer.10.attention.self.value.bias\n",
            "Append bert.encoder.layer.10.attention.output.dense.weight\n",
            "Append bert.encoder.layer.10.attention.output.dense.bias\n",
            "Append bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.10.intermediate.dense.weight\n",
            "Append bert.encoder.layer.10.intermediate.dense.bias\n",
            "Append bert.encoder.layer.10.output.dense.weight\n",
            "Append bert.encoder.layer.10.output.dense.bias\n",
            "Append bert.encoder.layer.10.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.10.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.11.attention.self.query.weight\n",
            "Append bert.encoder.layer.11.attention.self.query.bias\n",
            "Append bert.encoder.layer.11.attention.self.key.weight\n",
            "Append bert.encoder.layer.11.attention.self.key.bias\n",
            "Append bert.encoder.layer.11.attention.self.value.weight\n",
            "Append bert.encoder.layer.11.attention.self.value.bias\n",
            "Append bert.encoder.layer.11.attention.output.dense.weight\n",
            "Append bert.encoder.layer.11.attention.output.dense.bias\n",
            "Append bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "Append bert.encoder.layer.11.intermediate.dense.weight\n",
            "Append bert.encoder.layer.11.intermediate.dense.bias\n",
            "Append bert.encoder.layer.11.output.dense.weight\n",
            "Append bert.encoder.layer.11.output.dense.bias\n",
            "Append bert.encoder.layer.11.output.LayerNorm.weight\n",
            "Append bert.encoder.layer.11.output.LayerNorm.bias\n",
            "Append bert.pooler.dense.weight\n",
            "Append bert.pooler.dense.bias\n",
            "Append aspect_branch_list.0.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.0.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.0.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.0.att_fc_in_2.weight\n",
            "Append aspect_branch_list.0.att_fc_in_2.bias\n",
            "Append aspect_branch_list.0.fc_out.weight\n",
            "Append aspect_branch_list.0.fc_out.bias\n",
            "Append aspect_branch_list.1.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.1.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.1.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.1.att_fc_in_2.weight\n",
            "Append aspect_branch_list.1.att_fc_in_2.bias\n",
            "Append aspect_branch_list.1.fc_out.weight\n",
            "Append aspect_branch_list.1.fc_out.bias\n",
            "Append aspect_branch_list.2.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.2.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.2.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.2.att_fc_in_2.weight\n",
            "Append aspect_branch_list.2.att_fc_in_2.bias\n",
            "Append aspect_branch_list.2.fc_out.weight\n",
            "Append aspect_branch_list.2.fc_out.bias\n",
            "Append aspect_branch_list.3.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.3.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.3.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.3.att_fc_in_2.weight\n",
            "Append aspect_branch_list.3.att_fc_in_2.bias\n",
            "Append aspect_branch_list.3.fc_out.weight\n",
            "Append aspect_branch_list.3.fc_out.bias\n",
            "Append aspect_branch_list.4.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.4.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.4.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.4.att_fc_in_2.weight\n",
            "Append aspect_branch_list.4.att_fc_in_2.bias\n",
            "Append aspect_branch_list.4.fc_out.weight\n",
            "Append aspect_branch_list.4.fc_out.bias\n",
            "Append aspect_branch_list.5.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.5.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.5.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.5.att_fc_in_2.weight\n",
            "Append aspect_branch_list.5.att_fc_in_2.bias\n",
            "Append aspect_branch_list.5.fc_out.weight\n",
            "Append aspect_branch_list.5.fc_out.bias\n",
            "Append aspect_branch_list.6.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.6.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.6.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.6.att_fc_in_2.weight\n",
            "Append aspect_branch_list.6.att_fc_in_2.bias\n",
            "Append aspect_branch_list.6.fc_out.weight\n",
            "Append aspect_branch_list.6.fc_out.bias\n",
            "Append aspect_branch_list.7.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.7.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.7.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.7.att_fc_in_2.weight\n",
            "Append aspect_branch_list.7.att_fc_in_2.bias\n",
            "Append aspect_branch_list.7.fc_out.weight\n",
            "Append aspect_branch_list.7.fc_out.bias\n",
            "Append aspect_branch_list.8.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.8.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.8.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.8.att_fc_in_2.weight\n",
            "Append aspect_branch_list.8.att_fc_in_2.bias\n",
            "Append aspect_branch_list.8.fc_out.weight\n",
            "Append aspect_branch_list.8.fc_out.bias\n",
            "Append aspect_branch_list.9.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.9.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.9.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.9.att_fc_in_2.weight\n",
            "Append aspect_branch_list.9.att_fc_in_2.bias\n",
            "Append aspect_branch_list.9.fc_out.weight\n",
            "Append aspect_branch_list.9.fc_out.bias\n",
            "Append aspect_branch_list.10.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.10.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.10.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.10.att_fc_in_2.weight\n",
            "Append aspect_branch_list.10.att_fc_in_2.bias\n",
            "Append aspect_branch_list.10.fc_out.weight\n",
            "Append aspect_branch_list.10.fc_out.bias\n",
            "Append aspect_branch_list.11.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.11.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.11.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.11.att_fc_in_2.weight\n",
            "Append aspect_branch_list.11.att_fc_in_2.bias\n",
            "Append aspect_branch_list.11.fc_out.weight\n",
            "Append aspect_branch_list.11.fc_out.bias\n",
            "Append aspect_branch_list.12.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.12.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.12.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.12.att_fc_in_2.weight\n",
            "Append aspect_branch_list.12.att_fc_in_2.bias\n",
            "Append aspect_branch_list.12.fc_out.weight\n",
            "Append aspect_branch_list.12.fc_out.bias\n",
            "Append aspect_branch_list.13.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.13.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.13.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.13.att_fc_in_2.weight\n",
            "Append aspect_branch_list.13.att_fc_in_2.bias\n",
            "Append aspect_branch_list.13.fc_out.weight\n",
            "Append aspect_branch_list.13.fc_out.bias\n",
            "Append aspect_branch_list.14.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.14.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.14.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.14.att_fc_in_2.weight\n",
            "Append aspect_branch_list.14.att_fc_in_2.bias\n",
            "Append aspect_branch_list.14.fc_out.weight\n",
            "Append aspect_branch_list.14.fc_out.bias\n",
            "Append aspect_branch_list.15.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.15.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.15.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.15.att_fc_in_2.weight\n",
            "Append aspect_branch_list.15.att_fc_in_2.bias\n",
            "Append aspect_branch_list.15.fc_out.weight\n",
            "Append aspect_branch_list.15.fc_out.bias\n",
            "Append aspect_branch_list.16.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.16.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.16.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.16.att_fc_in_2.weight\n",
            "Append aspect_branch_list.16.att_fc_in_2.bias\n",
            "Append aspect_branch_list.16.fc_out.weight\n",
            "Append aspect_branch_list.16.fc_out.bias\n",
            "Append aspect_branch_list.17.attention_pooling.att_fc_in_1.weight\n",
            "Append aspect_branch_list.17.attention_pooling.att_fc_in_1.bias\n",
            "Append aspect_branch_list.17.attention_pooling.att_fc_out.weight\n",
            "Append aspect_branch_list.17.att_fc_in_2.weight\n",
            "Append aspect_branch_list.17.att_fc_in_2.bias\n",
            "Append aspect_branch_list.17.fc_out.weight\n",
            "Append aspect_branch_list.17.fc_out.bias\n",
            "Divide bert.embeddings.position_ids\n",
            "Divide bert.embeddings.word_embeddings.weight\n",
            "Divide bert.embeddings.position_embeddings.weight\n",
            "Divide bert.embeddings.token_type_embeddings.weight\n",
            "Divide bert.embeddings.LayerNorm.weight\n",
            "Divide bert.embeddings.LayerNorm.bias\n",
            "Divide bert.encoder.layer.0.attention.self.query.weight\n",
            "Divide bert.encoder.layer.0.attention.self.query.bias\n",
            "Divide bert.encoder.layer.0.attention.self.key.weight\n",
            "Divide bert.encoder.layer.0.attention.self.key.bias\n",
            "Divide bert.encoder.layer.0.attention.self.value.weight\n",
            "Divide bert.encoder.layer.0.attention.self.value.bias\n",
            "Divide bert.encoder.layer.0.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.0.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.0.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.0.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.0.output.dense.weight\n",
            "Divide bert.encoder.layer.0.output.dense.bias\n",
            "Divide bert.encoder.layer.0.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.0.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.1.attention.self.query.weight\n",
            "Divide bert.encoder.layer.1.attention.self.query.bias\n",
            "Divide bert.encoder.layer.1.attention.self.key.weight\n",
            "Divide bert.encoder.layer.1.attention.self.key.bias\n",
            "Divide bert.encoder.layer.1.attention.self.value.weight\n",
            "Divide bert.encoder.layer.1.attention.self.value.bias\n",
            "Divide bert.encoder.layer.1.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.1.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.1.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.1.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.1.output.dense.weight\n",
            "Divide bert.encoder.layer.1.output.dense.bias\n",
            "Divide bert.encoder.layer.1.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.1.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.2.attention.self.query.weight\n",
            "Divide bert.encoder.layer.2.attention.self.query.bias\n",
            "Divide bert.encoder.layer.2.attention.self.key.weight\n",
            "Divide bert.encoder.layer.2.attention.self.key.bias\n",
            "Divide bert.encoder.layer.2.attention.self.value.weight\n",
            "Divide bert.encoder.layer.2.attention.self.value.bias\n",
            "Divide bert.encoder.layer.2.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.2.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.2.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.2.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.2.output.dense.weight\n",
            "Divide bert.encoder.layer.2.output.dense.bias\n",
            "Divide bert.encoder.layer.2.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.2.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.3.attention.self.query.weight\n",
            "Divide bert.encoder.layer.3.attention.self.query.bias\n",
            "Divide bert.encoder.layer.3.attention.self.key.weight\n",
            "Divide bert.encoder.layer.3.attention.self.key.bias\n",
            "Divide bert.encoder.layer.3.attention.self.value.weight\n",
            "Divide bert.encoder.layer.3.attention.self.value.bias\n",
            "Divide bert.encoder.layer.3.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.3.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.3.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.3.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.3.output.dense.weight\n",
            "Divide bert.encoder.layer.3.output.dense.bias\n",
            "Divide bert.encoder.layer.3.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.3.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.4.attention.self.query.weight\n",
            "Divide bert.encoder.layer.4.attention.self.query.bias\n",
            "Divide bert.encoder.layer.4.attention.self.key.weight\n",
            "Divide bert.encoder.layer.4.attention.self.key.bias\n",
            "Divide bert.encoder.layer.4.attention.self.value.weight\n",
            "Divide bert.encoder.layer.4.attention.self.value.bias\n",
            "Divide bert.encoder.layer.4.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.4.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.4.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.4.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.4.output.dense.weight\n",
            "Divide bert.encoder.layer.4.output.dense.bias\n",
            "Divide bert.encoder.layer.4.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.4.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.5.attention.self.query.weight\n",
            "Divide bert.encoder.layer.5.attention.self.query.bias\n",
            "Divide bert.encoder.layer.5.attention.self.key.weight\n",
            "Divide bert.encoder.layer.5.attention.self.key.bias\n",
            "Divide bert.encoder.layer.5.attention.self.value.weight\n",
            "Divide bert.encoder.layer.5.attention.self.value.bias\n",
            "Divide bert.encoder.layer.5.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.5.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.5.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.5.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.5.output.dense.weight\n",
            "Divide bert.encoder.layer.5.output.dense.bias\n",
            "Divide bert.encoder.layer.5.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.5.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.6.attention.self.query.weight\n",
            "Divide bert.encoder.layer.6.attention.self.query.bias\n",
            "Divide bert.encoder.layer.6.attention.self.key.weight\n",
            "Divide bert.encoder.layer.6.attention.self.key.bias\n",
            "Divide bert.encoder.layer.6.attention.self.value.weight\n",
            "Divide bert.encoder.layer.6.attention.self.value.bias\n",
            "Divide bert.encoder.layer.6.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.6.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.6.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.6.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.6.output.dense.weight\n",
            "Divide bert.encoder.layer.6.output.dense.bias\n",
            "Divide bert.encoder.layer.6.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.6.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.7.attention.self.query.weight\n",
            "Divide bert.encoder.layer.7.attention.self.query.bias\n",
            "Divide bert.encoder.layer.7.attention.self.key.weight\n",
            "Divide bert.encoder.layer.7.attention.self.key.bias\n",
            "Divide bert.encoder.layer.7.attention.self.value.weight\n",
            "Divide bert.encoder.layer.7.attention.self.value.bias\n",
            "Divide bert.encoder.layer.7.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.7.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.7.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.7.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.7.output.dense.weight\n",
            "Divide bert.encoder.layer.7.output.dense.bias\n",
            "Divide bert.encoder.layer.7.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.7.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.8.attention.self.query.weight\n",
            "Divide bert.encoder.layer.8.attention.self.query.bias\n",
            "Divide bert.encoder.layer.8.attention.self.key.weight\n",
            "Divide bert.encoder.layer.8.attention.self.key.bias\n",
            "Divide bert.encoder.layer.8.attention.self.value.weight\n",
            "Divide bert.encoder.layer.8.attention.self.value.bias\n",
            "Divide bert.encoder.layer.8.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.8.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.8.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.8.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.8.output.dense.weight\n",
            "Divide bert.encoder.layer.8.output.dense.bias\n",
            "Divide bert.encoder.layer.8.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.8.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.9.attention.self.query.weight\n",
            "Divide bert.encoder.layer.9.attention.self.query.bias\n",
            "Divide bert.encoder.layer.9.attention.self.key.weight\n",
            "Divide bert.encoder.layer.9.attention.self.key.bias\n",
            "Divide bert.encoder.layer.9.attention.self.value.weight\n",
            "Divide bert.encoder.layer.9.attention.self.value.bias\n",
            "Divide bert.encoder.layer.9.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.9.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.9.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.9.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.9.output.dense.weight\n",
            "Divide bert.encoder.layer.9.output.dense.bias\n",
            "Divide bert.encoder.layer.9.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.9.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.10.attention.self.query.weight\n",
            "Divide bert.encoder.layer.10.attention.self.query.bias\n",
            "Divide bert.encoder.layer.10.attention.self.key.weight\n",
            "Divide bert.encoder.layer.10.attention.self.key.bias\n",
            "Divide bert.encoder.layer.10.attention.self.value.weight\n",
            "Divide bert.encoder.layer.10.attention.self.value.bias\n",
            "Divide bert.encoder.layer.10.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.10.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.10.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.10.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.10.output.dense.weight\n",
            "Divide bert.encoder.layer.10.output.dense.bias\n",
            "Divide bert.encoder.layer.10.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.10.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.11.attention.self.query.weight\n",
            "Divide bert.encoder.layer.11.attention.self.query.bias\n",
            "Divide bert.encoder.layer.11.attention.self.key.weight\n",
            "Divide bert.encoder.layer.11.attention.self.key.bias\n",
            "Divide bert.encoder.layer.11.attention.self.value.weight\n",
            "Divide bert.encoder.layer.11.attention.self.value.bias\n",
            "Divide bert.encoder.layer.11.attention.output.dense.weight\n",
            "Divide bert.encoder.layer.11.attention.output.dense.bias\n",
            "Divide bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "Divide bert.encoder.layer.11.intermediate.dense.weight\n",
            "Divide bert.encoder.layer.11.intermediate.dense.bias\n",
            "Divide bert.encoder.layer.11.output.dense.weight\n",
            "Divide bert.encoder.layer.11.output.dense.bias\n",
            "Divide bert.encoder.layer.11.output.LayerNorm.weight\n",
            "Divide bert.encoder.layer.11.output.LayerNorm.bias\n",
            "Divide bert.pooler.dense.weight\n",
            "Divide bert.pooler.dense.bias\n",
            "Divide aspect_branch_list.0.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.0.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.0.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.0.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.0.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.0.fc_out.weight\n",
            "Divide aspect_branch_list.0.fc_out.bias\n",
            "Divide aspect_branch_list.1.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.1.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.1.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.1.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.1.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.1.fc_out.weight\n",
            "Divide aspect_branch_list.1.fc_out.bias\n",
            "Divide aspect_branch_list.2.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.2.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.2.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.2.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.2.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.2.fc_out.weight\n",
            "Divide aspect_branch_list.2.fc_out.bias\n",
            "Divide aspect_branch_list.3.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.3.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.3.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.3.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.3.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.3.fc_out.weight\n",
            "Divide aspect_branch_list.3.fc_out.bias\n",
            "Divide aspect_branch_list.4.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.4.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.4.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.4.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.4.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.4.fc_out.weight\n",
            "Divide aspect_branch_list.4.fc_out.bias\n",
            "Divide aspect_branch_list.5.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.5.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.5.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.5.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.5.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.5.fc_out.weight\n",
            "Divide aspect_branch_list.5.fc_out.bias\n",
            "Divide aspect_branch_list.6.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.6.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.6.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.6.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.6.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.6.fc_out.weight\n",
            "Divide aspect_branch_list.6.fc_out.bias\n",
            "Divide aspect_branch_list.7.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.7.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.7.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.7.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.7.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.7.fc_out.weight\n",
            "Divide aspect_branch_list.7.fc_out.bias\n",
            "Divide aspect_branch_list.8.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.8.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.8.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.8.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.8.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.8.fc_out.weight\n",
            "Divide aspect_branch_list.8.fc_out.bias\n",
            "Divide aspect_branch_list.9.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.9.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.9.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.9.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.9.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.9.fc_out.weight\n",
            "Divide aspect_branch_list.9.fc_out.bias\n",
            "Divide aspect_branch_list.10.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.10.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.10.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.10.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.10.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.10.fc_out.weight\n",
            "Divide aspect_branch_list.10.fc_out.bias\n",
            "Divide aspect_branch_list.11.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.11.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.11.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.11.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.11.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.11.fc_out.weight\n",
            "Divide aspect_branch_list.11.fc_out.bias\n",
            "Divide aspect_branch_list.12.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.12.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.12.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.12.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.12.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.12.fc_out.weight\n",
            "Divide aspect_branch_list.12.fc_out.bias\n",
            "Divide aspect_branch_list.13.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.13.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.13.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.13.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.13.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.13.fc_out.weight\n",
            "Divide aspect_branch_list.13.fc_out.bias\n",
            "Divide aspect_branch_list.14.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.14.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.14.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.14.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.14.att_fc_in_2.bias\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:590.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Divide aspect_branch_list.14.fc_out.weight\n",
            "Divide aspect_branch_list.14.fc_out.bias\n",
            "Divide aspect_branch_list.15.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.15.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.15.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.15.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.15.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.15.fc_out.weight\n",
            "Divide aspect_branch_list.15.fc_out.bias\n",
            "Divide aspect_branch_list.16.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.16.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.16.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.16.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.16.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.16.fc_out.weight\n",
            "Divide aspect_branch_list.16.fc_out.bias\n",
            "Divide aspect_branch_list.17.attention_pooling.att_fc_in_1.weight\n",
            "Divide aspect_branch_list.17.attention_pooling.att_fc_in_1.bias\n",
            "Divide aspect_branch_list.17.attention_pooling.att_fc_out.weight\n",
            "Divide aspect_branch_list.17.att_fc_in_2.weight\n",
            "Divide aspect_branch_list.17.att_fc_in_2.bias\n",
            "Divide aspect_branch_list.17.fc_out.weight\n",
            "Divide aspect_branch_list.17.fc_out.bias\n",
            "Saving model NEW_model.ckpt\n"
          ]
        }
      ]
    }
  ]
}